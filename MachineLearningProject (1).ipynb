{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1:\n",
        "  - Load the data\n",
        "  - Define features to drop, in this case we want to ignore goal keeper stats, and other miscillaneous features.\n",
        "  - Use one-hot encoding for categorical features we want, like position\n",
        "  - Apply PCA\n"
      ],
      "metadata": {
        "id": "HIMAGLEWwNnw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNIqUSw0kp7o",
        "outputId": "b3398954-934c-446c-91e4-8a1ac8082f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1:\n",
            "\tcarries\n",
            "\tpasses_pressure\n",
            "\tpass_targets\n",
            "Principal Component 2:\n",
            "\ttouches_def_3rd\n",
            "\tnpxg_xa_per90\n",
            "\txg_xa_per90\n",
            "Principal Component 3:\n",
            "\tpasses_dead\n",
            "\tpasses_other_body\n",
            "\tshots_on_target_against\n",
            "Principal Component 4:\n",
            "\tshots_on_target_against\n",
            "\tpasses_other_body\n",
            "\ttouches_def_pen_area\n",
            "Principal Component 5:\n",
            "\tgoals_pens_per90\n",
            "\tgoals_assists_per90\n",
            "\tgoals_assists_pens_per90\n",
            "Principal Component 6:\n",
            "\tpasses_pct_medium\n",
            "\tpasses_pct_long\n",
            "\tpasses_pct\n",
            "Principal Component 7:\n",
            "\tcorner_kicks\n",
            "\tcrosses_into_penalty_area\n",
            "\tthrow_ins\n",
            "Principal Component 8:\n",
            "\tthrow_ins\n",
            "\tpressures_mid_3rd\n",
            "\tposition\n",
            "Principal Component 9:\n",
            "\tsca_per90\n",
            "\tgoals_per_shot\n",
            "\tshots_total_per90\n",
            "Principal Component 10:\n",
            "\tassists\n",
            "\tassists_per90\n",
            "\txa_net\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the data\n",
        "url = 'https://raw.githubusercontent.com/liam-rich/transfermarket_data/main/transfermarkt_fbref_201920.csv'\n",
        "data = pd.read_csv(url, delimiter=';', engine='python')\n",
        "\n",
        "# Features to drop\n",
        "bad_features = ['saves', 'save_pct', 'clean_sheets', 'clean_sheets_pct', 'pens_allowed', 'pens_saved', 'psnpxg_per_shot_on_target_against', 'goal_kicks','pct_goal_kicks_launched', 'xGDiff/90', 'xGDiff', 'xGA', 'xG', 'Pts/G', 'Pts', 'GDiff', 'GA', 'GF', 'L', 'D', 'W', 'MP', 'LgRk', 'xg_net', 'npxg_net', 'goal_kick_length_avg']\n",
        "data.drop(columns=bad_features, inplace=True)\n",
        "\n",
        "# Define non-numeric columns to drop\n",
        "non_numeric_columns_to_drop = ['player', 'foot', 'position2', 'Season', 'Attendance', 'Column1']\n",
        "data.drop(columns=non_numeric_columns_to_drop, inplace=True)\n",
        "\n",
        "# Define the categorical columns to encode\n",
        "categorical_columns = ['squad', 'nationality', 'position', 'league']\n",
        "\n",
        "# Initialize a LabelEncoder for each categorical column\n",
        "label_encoders = {}\n",
        "\n",
        "# Encode each categorical column\n",
        "for col in categorical_columns:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    data[col] = label_encoders[col].fit_transform(data[col])\n",
        "\n",
        "# Drop rows with NaN values consistently for X and y\n",
        "data.dropna(subset=['value'], inplace=True)\n",
        "\n",
        "# Filter out rows with \"GK\" in the position column\n",
        "data = data[data['position'] != 'GK']\n",
        "\n",
        "# For 'CLBestScorer', fill with 0 (assuming no goals implies not applicable/missing)\n",
        "data['CLBestScorer'].fillna(0, inplace=True)\n",
        "\n",
        "# Identify and drop columns ending in 'gk'\n",
        "gk_columns = [col for col in data.columns if col.endswith('gk')]\n",
        "data.drop(gk_columns, axis=1, inplace=True)\n",
        "\n",
        "# Define a list of 'm' columns to keep\n",
        "keep_columns = ['passes_completed_medium', 'passes_medium', 'passes_pct_medium']\n",
        "\n",
        "# Optionally, identify columns that end with 'm' but are not in 'keep_columns' to review or remove\n",
        "m_columns = [col for col in data.columns if col.endswith('m') and col not in keep_columns]\n",
        "data.drop(m_columns, axis=1, inplace=True)\n",
        "\n",
        "# Ensure your data has no missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop(columns=['value'])\n",
        "y = data['value']\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce dimensionality\n",
        "pca = PCA(n_components=0.70)  # Retain 70% of variance\n",
        "X_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Extract the principal component loadings\n",
        "loadings = pca.components_\n",
        "\n",
        "# Get the absolute values of the loadings\n",
        "abs_loadings = np.abs(loadings)\n",
        "\n",
        "# Find the indices of the features with the highest loadings for each principal component\n",
        "top_features_indices = np.argsort(abs_loadings, axis=1)[:, -3:]  # Choose top 3 features per component\n",
        "\n",
        "# Get the names of the original features\n",
        "feature_names = X.columns.values\n",
        "\n",
        "# Display the top features for each principal component\n",
        "for i, component in enumerate(top_features_indices):\n",
        "    print(f\"Principal Component {i+1}:\")\n",
        "    for feature_index in component:\n",
        "        print(f\"\\t{feature_names[feature_index]}\")\n",
        "\n",
        "# Use only the top features for modeling\n",
        "X_top_features = X.iloc[:, top_features_indices.flatten()]\n",
        "\n",
        "\n",
        "\n",
        "# Now we can proceed with modeling using X_top_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2:\n",
        "  - Linear regression\n",
        "  - Linear regression with polynomial features\n",
        "  - Ridge Regression\n",
        "    - Compare"
      ],
      "metadata": {
        "id": "CZeb31E-wpIS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lIyJLbJ_h07"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming 'X' and 'y' have been defined as in your previous code snippet\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_top_features, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature Engineering: Add interaction terms or polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly_train = poly.fit_transform(X_train)\n",
        "\n",
        "# Hyperparameter Tuning for Random Forest Regression\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=5)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "best_params_rf = grid_search_rf.best_params_\n",
        "\n",
        "# Model Selection: Random Forest Regression\n",
        "rf_model = RandomForestRegressor(**best_params_rf, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate Random Forest Model\n",
        "rf_y_pred = rf_model.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
        "print(f\"Random Forest MSE: {rf_mse}\")\n",
        "\n",
        "# Evaluate Linear Regression Model with Polynomial Features\n",
        "lr_poly = LinearRegression()\n",
        "lr_poly.fit(X_poly_train, y_train)\n",
        "X_poly_test = poly.transform(X_test)\n",
        "lr_y_pred_poly = lr_poly.predict(X_poly_test)\n",
        "lr_mse_poly = mean_squared_error(y_test, lr_y_pred_poly)\n",
        "print(f\"Polynomial Regression MSE: {lr_mse_poly}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}